{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ эффективности удержания\n",
    "В этом задании вам предлагается проанализировать данные одной из американских телекоммуникационных компаний о пользователях, которые потенциально могут уйти.\n",
    "\n",
    "Измерены следующие признаки:\n",
    "* state — штат США\n",
    "* account_length — длительность использования аккаунта\n",
    "* area_code — деление пользователей на псевдорегионы, использующееся в телекоме\n",
    "* intl_plan — подключена ли у пользователя услуга международного общения\n",
    "* vmail_plan — подключена ли у пользователя услуга голосовых сообщений\n",
    "* vmail_message — количество голосых сообщений, который пользователь отправил / принял\n",
    "* day_calls — сколько пользователь совершил дневных звонков\n",
    "* day_mins — сколько пользователь проговорил минут в течение дня\n",
    "* day_charge — сколько пользователь заплатил за свою дневную активность\n",
    "* eve_calls, eve_mins, eve_charge — аналогичные метрики относительно вечерней активности\n",
    "* night_calls, night_mins, night_charge — аналогичные метрики относительно ночной активности\n",
    "* intl_calls, intl_mins, intl_charge — аналогичные метрики относительно международного общения\n",
    "* custserv_calls — сколько раз пользователь позвонил в службу поддержки\n",
    "* treatment — номер стратегии, которая применялись для удержания абонентов (0, 2 = два разных типа воздействия, 1 = контрольная группа)\n",
    "* mes_estim — оценка интенсивности пользования интернет мессенджерами\n",
    "* churn — результат оттока: перестал ли абонент пользоваться услугами оператора\n",
    "\n",
    "Задание:\n",
    "1. Давайте рассмотрим всех пользователей из контрольной группы (treatment = 1). Для таких пользователей мы хотим проверить гипотезу о том, что штат абонента не влияет на то, перестанет ли абонент пользоваться услугами оператора. Постройте таблицы сопряженности между каждой из всех 1275 возможных неупорядоченных пар штатов и значением признака churn. Заметьте, что, например, (AZ, HI) и (HI, AZ) — это одна и та же пара. Какой критерий подходит для решения этой задачи? Сколько достигаемых уровней значимости оказались меньше, чем α=0.05?\n",
    "2. Посчитайте корреляции Пирсона и Спирмена между day_calls и mes_estim на всех данных, оцените их значимость, дайте интерпретацию результата.\n",
    "3. Посчитайте значение коэффицента корреляции Крамера между штатом (state) и оттоком пользователей (churn) для всех пользователей, которые находились в контрольной группе (treatment=1). Проверьте гипотезу об отсутствии связи между этими признаками.\n",
    "4. Проведите анализ эффективности удержания (churn) с помощью раличных методов (treatment = 0, treatment = 2) относительно контрольной группы пользователей (treatment = 1). Что можно сказать об этих двух методах (treatment = 0, treatment = 2)? Одинаковы ли они с точки зрения эффективности? Каким бы методом вы бы посоветовали воспользоваться компании?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>vmail_plan</th>\n",
       "      <th>vmail_message</th>\n",
       "      <th>day_mins</th>\n",
       "      <th>day_calls</th>\n",
       "      <th>day_charge</th>\n",
       "      <th>eve_mins</th>\n",
       "      <th>eve_calls</th>\n",
       "      <th>eve_charge</th>\n",
       "      <th>night_mins</th>\n",
       "      <th>night_calls</th>\n",
       "      <th>night_charge</th>\n",
       "      <th>intl_mins</th>\n",
       "      <th>intl_calls</th>\n",
       "      <th>intl_charge</th>\n",
       "      <th>custserv_calls</th>\n",
       "      <th>treatment</th>\n",
       "      <th>mes_estim</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account_length  area_code  intl_plan  vmail_plan  vmail_message  \\\n",
       "0    KS             128        415          0           1             25   \n",
       "1    OH             107        415          0           1             26   \n",
       "2    NJ             137        415          0           0              0   \n",
       "3    OH              84        408          1           0              0   \n",
       "4    OK              75        415          1           0              0   \n",
       "\n",
       "   day_mins  day_calls  day_charge  eve_mins  eve_calls  eve_charge  \\\n",
       "0     265.1        110       45.07     197.4         99       16.78   \n",
       "1     161.6        123       27.47     195.5        103       16.62   \n",
       "2     243.4        114       41.38     121.2        110       10.30   \n",
       "3     299.4         71       50.90      61.9         88        5.26   \n",
       "4     166.7        113       28.34     148.3        122       12.61   \n",
       "\n",
       "   night_mins  night_calls  night_charge  intl_mins  intl_calls  intl_charge  \\\n",
       "0       244.7           91         11.01       10.0           3         2.70   \n",
       "1       254.4          103         11.45       13.7           3         3.70   \n",
       "2       162.6          104          7.32       12.2           5         3.29   \n",
       "3       196.9           89          8.86        6.6           7         1.78   \n",
       "4       186.9          121          8.41       10.1           3         2.73   \n",
       "\n",
       "   custserv_calls  treatment  mes_estim  churn  \n",
       "0               1          1       0.65      0  \n",
       "1               1          0       0.55      0  \n",
       "2               0          0       0.72      0  \n",
       "3               2          1       0.28      0  \n",
       "4               3          2       0.45      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "    binary_features = ['intl_plan', 'vmail_plan', 'churn']\n",
    "    binary_mapper = {'yes': 1, 'True.': 1, 'no': 0, 'False.': 0}\n",
    "    for f in binary_features:\n",
    "        data[f] = data[f].map(binary_mapper)\n",
    "    return data\n",
    "\n",
    "df = read_data('churn_analysis.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>HI</th>\n",
       "      <th>AK</th>\n",
       "      <th>AZ</th>\n",
       "      <th>VA</th>\n",
       "      <th>IA</th>\n",
       "      <th>LA</th>\n",
       "      <th>NE</th>\n",
       "      <th>IL</th>\n",
       "      <th>WI</th>\n",
       "      <th>RI</th>\n",
       "      <th>DC</th>\n",
       "      <th>TN</th>\n",
       "      <th>WV</th>\n",
       "      <th>NM</th>\n",
       "      <th>ND</th>\n",
       "      <th>AL</th>\n",
       "      <th>VT</th>\n",
       "      <th>MO</th>\n",
       "      <th>WY</th>\n",
       "      <th>ID</th>\n",
       "      <th>IN</th>\n",
       "      <th>FL</th>\n",
       "      <th>OH</th>\n",
       "      <th>SD</th>\n",
       "      <th>KY</th>\n",
       "      <th>CO</th>\n",
       "      <th>UT</th>\n",
       "      <th>OR</th>\n",
       "      <th>OK</th>\n",
       "      <th>DE</th>\n",
       "      <th>GA</th>\n",
       "      <th>NH</th>\n",
       "      <th>NC</th>\n",
       "      <th>CT</th>\n",
       "      <th>MA</th>\n",
       "      <th>PA</th>\n",
       "      <th>MN</th>\n",
       "      <th>NY</th>\n",
       "      <th>KS</th>\n",
       "      <th>AR</th>\n",
       "      <th>MT</th>\n",
       "      <th>ME</th>\n",
       "      <th>WA</th>\n",
       "      <th>NV</th>\n",
       "      <th>MS</th>\n",
       "      <th>MI</th>\n",
       "      <th>SC</th>\n",
       "      <th>MD</th>\n",
       "      <th>TX</th>\n",
       "      <th>NJ</th>\n",
       "      <th>CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">churn</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.264706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>64.0000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>53.00000</td>\n",
       "      <td>106.00000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "state               HI         AK       AZ         VA         IA         LA  \\\n",
       "churn mean    0.056604   0.057692   0.0625   0.064935   0.068182   0.078431   \n",
       "      sum     3.000000   3.000000   4.0000   5.000000   3.000000   4.000000   \n",
       "      count  53.000000  52.000000  64.0000  77.000000  44.000000  51.000000   \n",
       "\n",
       "state               NE         IL         WI         RI         DC        TN  \\\n",
       "churn mean    0.081967   0.086207   0.089744   0.092308   0.092593   0.09434   \n",
       "      sum     5.000000   5.000000   7.000000   6.000000   5.000000   5.00000   \n",
       "      count  61.000000  58.000000  78.000000  65.000000  54.000000  53.00000   \n",
       "\n",
       "state               WV         NM         ND    AL         VT         MO  \\\n",
       "churn mean     0.09434   0.096774   0.096774   0.1   0.109589   0.111111   \n",
       "      sum     10.00000   6.000000   6.000000   8.0   8.000000   7.000000   \n",
       "      count  106.00000  62.000000  62.000000  80.0  73.000000  63.000000   \n",
       "\n",
       "state               WY         ID         IN         FL         OH         SD  \\\n",
       "churn mean    0.116883   0.123288   0.126761   0.126984   0.128205   0.133333   \n",
       "      sum     9.000000   9.000000   9.000000   8.000000  10.000000   8.000000   \n",
       "      count  77.000000  73.000000  71.000000  63.000000  78.000000  60.000000   \n",
       "\n",
       "state               KY         CO         UT         OR         OK         DE  \\\n",
       "churn mean    0.135593   0.136364   0.138889   0.141026   0.147541   0.147541   \n",
       "      sum     8.000000   9.000000  10.000000  11.000000   9.000000   9.000000   \n",
       "      count  59.000000  66.000000  72.000000  78.000000  61.000000  61.000000   \n",
       "\n",
       "state               GA         NH         NC         CT         MA         PA  \\\n",
       "churn mean    0.148148   0.160714   0.161765   0.162162   0.169231   0.177778   \n",
       "      sum     8.000000   9.000000  11.000000  12.000000  11.000000   8.000000   \n",
       "      count  54.000000  56.000000  68.000000  74.000000  65.000000  45.000000   \n",
       "\n",
       "state               MN         NY         KS    AR         MT         ME  \\\n",
       "churn mean    0.178571   0.180723   0.185714   0.2   0.205882   0.209677   \n",
       "      sum    15.000000  15.000000  13.000000  11.0  14.000000  13.000000   \n",
       "      count  84.000000  83.000000  70.000000  55.0  68.000000  62.000000   \n",
       "\n",
       "state               WA         NV         MS         MI         SC         MD  \\\n",
       "churn mean    0.212121   0.212121   0.215385   0.219178   0.233333   0.242857   \n",
       "      sum    14.000000  14.000000  14.000000  16.000000  14.000000  17.000000   \n",
       "      count  66.000000  66.000000  65.000000  73.000000  60.000000  70.000000   \n",
       "\n",
       "state           TX         NJ         CA  \n",
       "churn mean    0.25   0.264706   0.264706  \n",
       "      sum    18.00  18.000000   9.000000  \n",
       "      count  72.00  68.000000  34.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.groupby('state').agg({\n",
    "    'churn': ['mean', 'sum','count']})).sort_values(by=('churn','mean')).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "Давайте рассмотрим всех пользователей из контрольной группы (treatment = 1). Для таких пользователей мы хотим проверить гипотезу о том, что штат абонента не влияет на то, перестанет ли абонент пользоваться услугами оператора. Постройте таблицы сопряженности между каждой из всех 1275 возможных неупорядоченных пар штатов и значением признака churn. Заметьте, что, например, (AZ, HI) и (HI, AZ) — это одна и та же пара. Какой критерий подходит для решения этой задачи? Сколько достигаемых уровней значимости оказались меньше, чем α=0.05\n",
    "\n",
    "**Для данной задачи лучше всего подходит точный критерий Фишера**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = df[df.treatment == 1]\n",
    "t1_cs = pd.crosstab(t1.state, t1.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn   0  1\n",
       "state       \n",
       "AK     19  1\n",
       "AL     25  5\n",
       "AR     11  5\n",
       "AZ     17  2\n",
       "CA     10  5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_cs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fisher_test(table, alternative='two-sided'):\n",
    "    pairs = [list(c) for c in combinations(table.index, 2)]\n",
    "    p_values = [fisher_exact(table.loc[c])[1] for c in pairs]\n",
    "    return np.array(p_values), pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_p_values, state_pairs = fisher_test(t1_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = pd.DataFrame({'states': state_pairs, 'p-value': state_p_values})\n",
    "for i, (s1, s2) in enumerate(states.states):\n",
    "    states.loc[i, 'state1'] = df[df.state == s1].churn.mean()\n",
    "    states.loc[i, 'state2'] = df[df.state == s2].churn.mean()\n",
    "states['diff'] = np.abs(states['state1'] - states['state2'])\n",
    "states.sort_values(by='p-value', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-value</th>\n",
       "      <th>states</th>\n",
       "      <th>state1</th>\n",
       "      <th>state2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0.026963</td>\n",
       "      <td>[LA, TX]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.171569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0.029057</td>\n",
       "      <td>[LA, WA]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.133690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.030730</td>\n",
       "      <td>[LA, MA]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.090799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.032840</td>\n",
       "      <td>[KS, LA]</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.107283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.035768</td>\n",
       "      <td>[LA, ME]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.131246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p-value    states    state1    state2      diff\n",
       "771  0.026963  [LA, TX]  0.078431  0.250000  0.171569\n",
       "775  0.029057  [LA, WA]  0.078431  0.212121  0.133690\n",
       "747  0.030730  [LA, MA]  0.078431  0.169231  0.090799\n",
       "681  0.032840  [KS, LA]  0.185714  0.078431  0.107283\n",
       "749  0.035768  [LA, ME]  0.078431  0.209677  0.131246"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не учитываем поправку на множественную проверку гипотез"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-value</th>\n",
       "      <th>states</th>\n",
       "      <th>state1</th>\n",
       "      <th>state2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026963</td>\n",
       "      <td>[LA, TX]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.171569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029057</td>\n",
       "      <td>[LA, WA]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.133690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030730</td>\n",
       "      <td>[LA, MA]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.090799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.032840</td>\n",
       "      <td>[KS, LA]</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.107283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035768</td>\n",
       "      <td>[LA, ME]</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.131246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.035772</td>\n",
       "      <td>[CA, NM]</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.167932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041122</td>\n",
       "      <td>[ME, NM]</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.112903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.042146</td>\n",
       "      <td>[CA, LA]</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.186275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.043382</td>\n",
       "      <td>[AR, LA]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.121569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.049676</td>\n",
       "      <td>[NM, TX]</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.153226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p-value    states    state1    state2      diff\n",
       "0  0.026963  [LA, TX]  0.078431  0.250000  0.171569\n",
       "1  0.029057  [LA, WA]  0.078431  0.212121  0.133690\n",
       "2  0.030730  [LA, MA]  0.078431  0.169231  0.090799\n",
       "3  0.032840  [KS, LA]  0.185714  0.078431  0.107283\n",
       "4  0.035768  [LA, ME]  0.078431  0.209677  0.131246\n",
       "5  0.035772  [CA, NM]  0.264706  0.096774  0.167932\n",
       "6  0.041122  [ME, NM]  0.209677  0.096774  0.112903\n",
       "7  0.042146  [CA, LA]  0.264706  0.078431  0.186275\n",
       "8  0.043382  [AR, LA]  0.200000  0.078431  0.121569\n",
       "9  0.049676  [NM, TX]  0.096774  0.250000  0.153226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_states = states[states['p-value'] < alpha]\n",
    "nmt_states.index = range(len(nmt_states))\n",
    "nmt_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы нашли 10 пар штатов, для которых доля ушедших абонентов статистически значимо отличается.\n",
    "\n",
    "Интересно, что в 7 случаях присутствует штат LA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь учтём поправку на множественную проверку гипотез"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holm_rejected, p_holm, _, _ = multipletests(state_p_values, alpha=alpha, method='holm')\n",
    "fdr_bh_rejected, p_fdr_bh, _, _ = multipletests(state_p_values, alpha=alpha, method='fdr_bh')\n",
    "mt_states = pd.DataFrame({'states': state_pairs, 'p-values': state_p_values, \n",
    "                          'p_holm': p_holm, 'p_fdr_bh': p_fdr_bh,\n",
    "                          'rejected': np.logical_or(holm_rejected, fdr_bh_rejected)\n",
    "                         }).sort_values(by='rejected', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>p_fdr_bh</th>\n",
       "      <th>p_holm</th>\n",
       "      <th>rejected</th>\n",
       "      <th>states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.381063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[AK, AL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.190296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ME, NE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.457671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ME, OK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.085544</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ME, OH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.374605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ME, NY]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p-values  p_fdr_bh  p_holm rejected    states\n",
       "0    0.381063       1.0     1.0    False  [AK, AL]\n",
       "847  0.190296       1.0     1.0    False  [ME, NE]\n",
       "854  0.457671       1.0     1.0    False  [ME, OK]\n",
       "853  0.085544       1.0     1.0    False  [ME, OH]\n",
       "852  0.374605       1.0     1.0    False  [ME, NY]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p_holm - p-value при использовании метода Холма\n",
    "# p_fdr_bh - p-value при использовании метода Бенджамини-Хохберга\n",
    "# rejected - Была ли отвергнута нулевая гипотеза  хотябы одним из методов\n",
    "mt_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из таблички (значения в ней отсортированы по убыванию признака rejected, который равен true, если нулевая гипотеза для этой пары штатов отвергается хотябы одной из поправок) любая из поправок на множественную проверку не позволяет отвергнуть нулевую гипотезу ни для одной пары штатов. Учитывая это, можно сделать сделать вывод, что штат не влияет на то, перестанет ли абонент пользоваться услугами оператора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про методы Холма и Бенджамини-Хохберга можно прочитать [здесь](https://link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "Посчитайте корреляции Пирсона и Спирмена между day_calls и mes_estim на всех данных, оцените их значимость, дайте интерпретацию результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr, probplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pearson_r, pearson_p = pearsonr(df.day_calls, df.mes_estim)\n",
    "spearman_r, spearman_p = spearmanr(df.day_calls, df.mes_estim)\n",
    "print(\"Pearson correlation: %f, p value: %f\" % (pearson_r, pearson_p))\n",
    "print(\"Spearman correlation: %f, p value: %f\" % (spearman_r, spearman_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих случаях нулевая гипотеза об отсутствии корреляции отвергается на уровне доверия 0.05.\n",
    "\n",
    "Интересно, что в случае корреляции Пирсона наблюдается (очень) слабая отрицательная связь, в то время как в случае корреляции Спирмена - (очень) слабая положительная связь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверим имеют ли признаки day_calls и mes_estim нормальное распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "sns.distplot(df.day_calls)\n",
    "\n",
    "plt.subplot(223)\n",
    "probplot(df.day_calls, dist='norm', plot=plt)\n",
    "plt.title('day_calls QQ plot')\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.distplot(df.mes_estim)\n",
    "\n",
    "plt.subplot(224)\n",
    "probplot(df.mes_estim, dist='norm', plot=plt)\n",
    "plt.title('mes_estim QQ plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Критерий Шапиро-Уилка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, day_calls_p = shapiro(df.day_calls)\n",
    "_, mes_estim_p = shapiro(df.mes_estim)\n",
    "print(\"day_calls p-value: %f\" % day_calls_p)\n",
    "print(\"mes_estim p-value: %f\" % mes_estim_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по гистограммам, QQ plot-ам и критерию Шапиро-Уилка признак day_calls действительно распределён нормально, а признак mes_estim имеет распределение близкое к нормальному."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Взглянем на scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='day_calls', y='mes_estim', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = (df.mes_estim > 0.24)\n",
    "print(\"Pearson correlation: %f, p value: %f\" % pearsonr(df.day_calls[mask], df.mes_estim[mask]))\n",
    "print(\"Spearman correlation: %f, p value: %f\" % spearmanr(df.day_calls[mask], df.mes_estim[mask]))\n",
    "sns.lmplot(data=df[mask], x='day_calls', y='mes_estim', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = (df.mes_estim < 0.24)\n",
    "print(\"Pearson correlation: %f, p value: %f\" % pearsonr(df.day_calls[mask], df.mes_estim[mask]))\n",
    "print(\"Spearman correlation: %f, p value: %f\" % spearmanr(df.day_calls[mask], df.mes_estim[mask]))\n",
    "sns.lmplot(data=df[mask], x='day_calls', y='mes_estim', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод: TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "Посчитайте значение коэффицента корреляции Крамера между штатом (state) и оттоком пользователей (churn) для всех пользователей, которые находились в контрольной группе (treatment=1). Проверьте гипотезу об отсутствии связи между этими признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramersV(contingency_table):\n",
    "    r, c = contingency_table.sum(axis=1), contingency_table.sum(axis=0)\n",
    "    n = contingency_table.sum().sum()\n",
    "    chi2, p, _, exp_freq = chi2_contingency(contingency_table, correction=False)\n",
    "    corr = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "    return corr, p, exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cramerv_r, cramerv_p, exp_freq = cramersV(t1_cs)\n",
    "print(\"Cramer's V: %f, p value: %f\" % (cramerv_r, cramerv_p))\n",
    "print(\"Cells with expected cell count >= 5: %f\" % (exp_freq >= 5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Вывод**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4\n",
    "Проведите анализ эффективности удержания (churn) с помощью раличных методов (treatment = 0, treatment = 2) относительно контрольной группы пользователей (treatment = 1). Что можно сказать об этих двух методах (treatment = 0, treatment = 2)? Одинаковы ли они с точки зрения эффективности? Каким бы методом вы бы посоветовали воспользоваться компании?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = df[df.treatment == 0]\n",
    "t2 = df[df.treatment == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'control group': t1.churn.mean(), 'treatment 0': t0.churn.mean(),\n",
    "              'treatment 2': t2.churn.mean()}, index=['churn rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-критерий для разности долей в независимых выборках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   | $X_1$ | $X_2$  \n",
    "  ------------- | -------------|\n",
    "  1  | a | b \n",
    "  0  | c | d \n",
    "  $\\sum$ | $n_1$| $n_2$\n",
    "  \n",
    "$$ \\hat{p}_1 = \\frac{a}{n_1}$$\n",
    "\n",
    "$$ \\hat{p}_2 = \\frac{b}{n_2}$$\n",
    "\n",
    "\n",
    "$$\\text{Доверительный интервал для }p_1 - p_2\\colon \\;\\; \\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}_1(1 - \\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1 - \\hat{p}_2)}{n_2}}$$\n",
    "\n",
    "$$Z-статистика: Z({X_1, X_2}) =  \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{P(1 - P)(\\frac{1}{n_1} + \\frac{1}{n_2})}}$$\n",
    "$$P = \\frac{\\hat{p}_1{n_1} + \\hat{p}_2{n_2}}{{n_1} + {n_2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportions_diff_confint_ind(sample1, sample2, alpha = 0.05):    \n",
    "    z = scipy.stats.norm.ppf(1 - alpha / 2.)\n",
    "    \n",
    "    p1 = float(sum(sample1)) / len(sample1)\n",
    "    p2 = float(sum(sample2)) / len(sample2)\n",
    "    \n",
    "    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    \n",
    "    return (left_boundary, right_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportions_diff_z_stat_ind(sample1, sample2):\n",
    "    n1 = len(sample1)\n",
    "    n2 = len(sample2)\n",
    "    \n",
    "    p1 = float(sum(sample1)) / n1\n",
    "    p2 = float(sum(sample2)) / n2 \n",
    "    P = float(p1*n1 + p2*n2) / (n1 + n2)\n",
    "    \n",
    "    return (p1 - p2) / np.sqrt(P * (1 - P) * (1. / n1 + 1. / n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportions_diff_z_test(sample1, sample2, alternative = 'two-sided'):\n",
    "    if alternative not in ('two-sided', 'less', 'greater'):\n",
    "        raise ValueError(\"alternative not recognized\\n\"\n",
    "                         \"should be 'two-sided', 'less' or 'greater'\")\n",
    "    \n",
    "    z_stat = proportions_diff_z_stat_ind(sample1, sample2)\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        return 2 * (1 - scipy.stats.norm.cdf(np.abs(z_stat)))\n",
    "    \n",
    "    if alternative == 'less':\n",
    "        return scipy.stats.norm.cdf(z_stat)\n",
    "\n",
    "    if alternative == 'greater':\n",
    "        return 1 - scipy.stats.norm.cdf(z_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proportions_diff_z_test(t1.churn, t0.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proportions_diff_z_test(t1.churn, t2.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proportions_diff_z_test(t0.churn, t2.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proportions_diff_confint_ind(t1.churn, t2.churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Критерий $\\chi ^{2}$ (хи-квадрат)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_cs = pd.crosstab(df.treatment, df.churn)\n",
    "t_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi2_contingency(t_cs.loc[[1,0]], correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi2_contingency(t_cs.loc[[1,2]], correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi2_contingency(t_cs.loc[[0,2]], correction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: interpretaion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Критерий Фишера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.treatment, df.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_value_ft, treatments_ft = fisher_test(pd.crosstab(df.treatment, df.churn))\n",
    "for p, t in zip(p_value_ft, treatments_ft):\n",
    "    print(t, \": \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, drop_features, cat_features):\n",
    "    y = data['churn'].astype(bool)\n",
    "    X = data.drop(drop_features + ['churn'], axis=1)\n",
    "    X = pd.get_dummies(X, columns=cat_features)\n",
    "    return train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_features = ['state' ,\n",
    "                 'day_charge', 'eve_charge', 'night_charge', 'intl_charge',\n",
    "                 'area_code', \n",
    "                 'vmail_plan']\n",
    "cat_features = ['treatment']\n",
    "\n",
    "X_train, X_val, y_train, y_val = prepare_data(df, drop_features, cat_features)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['custserv_calls >= 4'] = X_train.custserv_calls >= 4\n",
    "X_val['custserv_calls >= 4'] = X_val.custserv_calls >= 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_val = poly.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(penalty='l2', class_weight='balanced', Cs=20,\n",
    "                          scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train: %f\" % roc_auc_score(y_train, lr.predict_proba(X_train)[:, 1:]))\n",
    "print(\"CV: %f\" % lr.scores_[True][0].mean())\n",
    "print(\"Test: %f\" % roc_auc_score(y_val, lr.predict_proba(X_val)[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lr.coef_[0], index=X_train.columns, columns=['coef']).sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_features = ['day_charge', 'eve_charge', 'night_charge', 'intl_charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = prepare_data(df, drop_features,\n",
    "                                              cat_features=['treatment', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = prepare_data(df, drop_features + ['state'],\n",
    "                                              cat_features=['treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': range(1,11), 'max_features': range(4,X_train.shape[1])}\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best params: %s\" % gs.best_params_)\n",
    "print(\"Train: %f\" % roc_auc_score(y_train, gs.best_estimator_.predict_proba(X_train)[:, 1:]))\n",
    "print(\"CV: %f\" % gs.best_score_)\n",
    "print(\"Test: %f\" % roc_auc_score(y_val, gs.best_estimator_.predict_proba(X_val)[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(gs.cv_results_)\n",
    "gs.cv_results_['std_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train: %f\" % roc_auc_score(y_train, dt.predict_proba(X_train)[:, 1:]))\n",
    "print(\"Test: %f\" % roc_auc_score(y_val, dt.predict_proba(X_val)[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.core.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(dt, feature_names=X_train.columns, out_file='tree.dot',\n",
    "                filled=True, label='root')\n",
    "!dot -Tpng 'tree.dot' -o 'tree.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Image('tree.png', unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[(X_train.day_mins <= 264.55) & (X_train.intl_plan == 1) & (X_train.custserv_calls > 3.5)].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgb_res = pd.DataFrame()\n",
    "\n",
    "for n in n_estimators:\n",
    "    lgb = lightgbm.LGBMClassifier(n_estimators=n)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    lgb_res.loc[n, 'train'] = roc_auc_score(y_train, lgb.predict_proba(X_train)[:, 1:])\n",
    "    lgb_res.loc[n, 'test'] = roc_auc_score(y_val, lgb.predict_proba(X_val)[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgb_res"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
